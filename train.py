import os
import numpy as np
from keras import layers, callbacks
import keras

import tensorflow as tf
from tensorflow.python.ops.metrics_impl import false_positives

devices = tf.config.list_physical_devices()
print("Available devices:", devices)

# Parameters
max_length = 20000  # Length to which sequences will be padded or truncated
model_save_path = "saved_model/malware_detection_model_20k.keras"  # Path to save the trained model

def load_executable(file_path):
    with open(file_path, 'rb') as f:
        data = f.read()
    return np.frombuffer(data, dtype=np.uint8)

def preprocess_files(directory, label, max_length):
    sequences = []
    labels = []
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        try:
            sequence = load_executable(file_path)
        except FileNotFoundError:
            continue
        sequence = sequence[:max_length]  # Truncate
        if len(sequence) < max_length:
            sequence = np.pad(sequence, (0, max_length - len(sequence)), 'constant')
        sequences.append(sequence)
        labels.append(label)
    return np.array(sequences), np.array(labels)

# Load the datasets
malware_sequences, malware_labels = preprocess_files('virus', 1, max_length)
benign_sequences, benign_labels = preprocess_files('benign', 0, max_length)
false_positive_sequences, false_positive_labels = preprocess_files('false_positives', 0, max_length)

# Combine and shuffle
X = np.concatenate((malware_sequences, benign_sequences, false_positive_sequences), axis=0)
y = np.concatenate((malware_labels, benign_labels, false_positive_labels), axis=0)

# Shuffle the dataset
shuffle_indices = np.random.permutation(len(X))
X, y = X[shuffle_indices], y[shuffle_indices]

# Split the dataset
split_index = int(0.8 * len(X))
X_train, X_test = X[:split_index], X[split_index:]
y_train, y_test = y[:split_index], y[split_index:]

# Create the model
model = keras.Sequential([
    layers.Input(shape=(max_length,)),
    layers.Embedding(input_dim=256, output_dim=128),  # Embed the byte values
    layers.Conv1D(64, 3, activation='relu'),
    layers.MaxPooling1D(pool_size=2),
    layers.Conv1D(128, 3, activation='relu'),
    layers.GlobalMaxPooling1D(),
    layers.Dense(128, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Callback for saving the best model
model_checkpoint = callbacks.ModelCheckpoint(
    filepath=model_save_path,
    save_best_only=True,
    monitor='val_loss',
    mode='min',
    verbose=1
)

class_weights = {0: 2.0, 1: 1.0}

# Train the model with progress information
history = model.fit(
    X_train,
    y_train,
    epochs=5,
    batch_size=64,
    class_weight=class_weights,
    validation_split=0.1,
    callbacks=[model_checkpoint],
    verbose=1  # Verbose 1 will show a progress bar with loss and accuracy per epoch
)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_acc:.4f}')

# Save the final model
model.save(model_save_path)
print(f"Model saved to {model_save_path}")